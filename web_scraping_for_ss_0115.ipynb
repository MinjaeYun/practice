{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web scraping for ss 0115.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgDpkprw9JKO"
      },
      "source": [
        "#**Web Scraping for Social Sciences**\r\n",
        "Minjae Yun </br> </br>\r\n",
        "[Text book](https://jakevdp.github.io/PythonDataScienceHandbook/)\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "##Final Goals\r\n",
        "*   Obtain and read url page \r\n",
        "*   Clean and stack information into a dataframe \r\n",
        "*   Repeat the same tasks\r\n",
        "*   Trials & errors\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "## Lecture 1 : Setting Up and Basics\r\n",
        "  Lecture 2 : Basic Web Scraping </br>\r\n",
        "  Lecture 3 : Advanced Web Scraping </br>\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "[Installing Anaconda](https://python-programming.quantecon.org/getting_started.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CTFuIGl_OqZ"
      },
      "source": [
        "### 1. Load modules\r\n",
        "[General installation method](https://docs.python.org/3/installing/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJOLVD1D9IgD"
      },
      "source": [
        "# !pip install BeautifulSoup\r\n",
        "# !pip install requests\r\n",
        "import os # operating system dependent functionality (set the working directory)\r\n",
        "import pandas as pd # main matrix operations library (also read files)\r\n",
        "import requests # get pages from web (no interactivity)\r\n",
        "from bs4 import BeautifulSoup # get pages from web (interactively)\r\n",
        "import re # regular expression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVNkyxnB8im"
      },
      "source": [
        "### 2. Notes on list, tuple, and dataframe\r\n",
        "* List is the most general container for a collection of information \r\n",
        "* Tuple also contains a collection of information but the values in it wouldn't change\r\n",
        "  * Developers use tuples when they wouldn't want to change the values e.g. homepage addresses (social scientists rarely use this)\r\n",
        "*  Pandas is a modeule for operating data structures which is motivated by R dataframe\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C44-oJVzCAPt"
      },
      "source": [
        "# list can contain anything\r\n",
        "a_list = [1,2,3,4,5] # numbers\r\n",
        "b_list = [\"a\",3,5,\"d\",\"e\"] # characters and numbers\r\n",
        "c_list = [a_list,\"b\",6,b_list,7,\"c\" ] # lists, characters and numbers  \r\n",
        "a_list = [1+x for x in a_list] # list operator\r\n",
        "a_tuple=(1,2,3,4,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keAiXcUuCcTd"
      },
      "source": [
        "## 3. Use modules\r\n",
        "Examples of obtaining information from web pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6XFp_MYCKlt"
      },
      "source": [
        "df = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIfyIu3PCO1m"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import requests\r\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_mental_disorders'\r\n",
        "s = requests.get(url)\r\n",
        "b = BeautifulSoup(s.text, 'lxml')\r\n",
        "#create an empty list where those list objects \r\n",
        "main_disorders = []\r\n",
        "\r\n",
        "# grab all of the li tags and store the text\r\n",
        "for i in b.find_all(name = 'li'):\r\n",
        "    main_disorders.append(i.text)\r\n",
        "#Subset out the ones we care about\r\n",
        "main_disorders = main_disorders[26:216]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hESInpSfCQc9",
        "outputId": "88cb5e5d-cbc8-4636-e25c-e4bda159a044"
      },
      "source": [
        "import json\r\n",
        "all_urls = ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/' + x + '/daily/2010010100/2019020900' for x in main_disorders]\r\n",
        "\r\n",
        "url = all_urls[0]\r\n",
        "s=requests.get(url).content\r\n",
        "json_string = s.decode('utf-8').replace(\"'\", \"\\\"\")\r\n",
        "d = json.loads(json_string)\r\n",
        "values = pd.DataFrame.from_dict(d['items'])\r\n",
        "values.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>project</th>\n",
              "      <th>article</th>\n",
              "      <th>granularity</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>access</th>\n",
              "      <th>agent</th>\n",
              "      <th>views</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en.wikipedia</td>\n",
              "      <td>Absence_seizure</td>\n",
              "      <td>daily</td>\n",
              "      <td>2015070100</td>\n",
              "      <td>all-access</td>\n",
              "      <td>user</td>\n",
              "      <td>636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en.wikipedia</td>\n",
              "      <td>Absence_seizure</td>\n",
              "      <td>daily</td>\n",
              "      <td>2015070200</td>\n",
              "      <td>all-access</td>\n",
              "      <td>user</td>\n",
              "      <td>592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en.wikipedia</td>\n",
              "      <td>Absence_seizure</td>\n",
              "      <td>daily</td>\n",
              "      <td>2015070300</td>\n",
              "      <td>all-access</td>\n",
              "      <td>user</td>\n",
              "      <td>460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en.wikipedia</td>\n",
              "      <td>Absence_seizure</td>\n",
              "      <td>daily</td>\n",
              "      <td>2015070400</td>\n",
              "      <td>all-access</td>\n",
              "      <td>user</td>\n",
              "      <td>454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en.wikipedia</td>\n",
              "      <td>Absence_seizure</td>\n",
              "      <td>daily</td>\n",
              "      <td>2015070500</td>\n",
              "      <td>all-access</td>\n",
              "      <td>user</td>\n",
              "      <td>488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        project          article granularity  ...      access agent views\n",
              "0  en.wikipedia  Absence_seizure       daily  ...  all-access  user   636\n",
              "1  en.wikipedia  Absence_seizure       daily  ...  all-access  user   592\n",
              "2  en.wikipedia  Absence_seizure       daily  ...  all-access  user   460\n",
              "3  en.wikipedia  Absence_seizure       daily  ...  all-access  user   454\n",
              "4  en.wikipedia  Absence_seizure       daily  ...  all-access  user   488\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtRZvmqnCz5U"
      },
      "source": [
        "## 4. Regular expression\r\n",
        "[Google education](https://developers.google.com/edu/python/regular-expressions) <br>\r\n",
        "[Practice page](https://regexr.com/) <br>\r\n",
        "We can capture abstract words by using the regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHQiJaJCy1v"
      },
      "source": [
        "import re\r\n",
        "text = \"take each type of chunks of characters from this string: @$%&* 18424 AZDFTFH kjiwer mail@address.com another@address.com theother@gmail.com\"\r\n",
        "print(re.findall(\"\\w+\", text)) # chracters except for symbols\r\n",
        "print(re.findall(\"[a-z]+\", text)) # only the lower letters\r\n",
        "print(re.findall(\"[0-9]+\", text)) # only the numbers"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
